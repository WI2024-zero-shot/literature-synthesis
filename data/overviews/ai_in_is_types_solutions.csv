title;link;abstract;type
A Machine Learning Approach to Improving Dynamic Decision Making;https://doi.org/10.1287/isre.2014.0513;Decision strategies in dynamic environments do not always succeed in producing desired outcomes, particularly in complex, ill-structured domains. Information systems often capture large amounts of data about such environments. We propose a domain-independent, iterative approach that (a) applies data mining classification techniques to the collected data in order to discover the conditions under which dynamic decision-making strategies produce undesired or suboptimal outcomes and (b) uses this information to improve the decision strategy under these conditions. In this paper, we formally develop this approach and illustrate it by providing detailed examples of its application to a chronic disease care problem in a healthcare management organization, specifically the treatment of patients with type 2 diabetes mellitus. In particular, the proposed iterative approach is used to improve treatment strategies by predicting and eliminating treatment failures, i.e., insufficient or excessive treatment actions, based on information that is available in electronic medical record systems. We also apply the proposed approach to a manufacturing task, resulting in substantial decision strategy improvements, which further demonstrates the generality and flexibility of the proposed approach.;machine learning
Examining the Impact of Keyword Ambiguity on Search Advertising Performance: A Topic Model Approach;https://misq.umn.edu/examining-the-impact-of-keyword-ambiguity-on-search-advertising-performance-a-topic-model-approach.html?SID=a8c3ub0214v89bip4a0qktvs05;In this paper, we explore how keyword ambiguity can affect search advertising performance. Consumers arrive at search engines with diverse interests, which are often unobserved and nontrivial to predict. The search interests of different consumers may vary even when they are searching using the same keyword. In our study, we propose an automatic way of examining keyword ambiguity based on probabilistic topic models from machine learning and computational linguistics. We examine the effect of keyword ambiguity on keyword performance using a hierarchical Bayesian approach that allows for topic-specific effects and nonlinear position effects, and jointly models click-through rate (CTR) and ad position (rank). We validate our study using a novel data set from a major search engine that contains information on consumer click activities for 2,625 distinct keywords across multiple product categories from 10,000 impressions. We find that consumer click behavior varies significantly across keywords, and such variation can be partially explained by keyword ambiguity. Specifically, higher keyword ambiguity is associated with higher CTR on top-positioned ads, but also a faster decay in CTR with screen position. Therefore, the overall effect of keyword ambiguity on CTR varies across positions. Our study provides implications for advertisers to improve the prediction of keyword performance by taking into account keyword ambiguity and other semantic characteristics of keywords. It can also help search engines design keyword planning tools to aid advertisers when choosing potential keywords.;machine learning
Expecting the Unexpected: Effects of Data Collection Design Choices on the Quality of Crowdsourced User-Generated Content;https://misq.umn.edu/expecting-the-unexpected-effects-of-data-collection-design-choices-on-the-quality-of-crowdsourced-user-generated-content.html;As crowdsourced user-generated content becomes an important source of data for organizations, a pressing question is how to ensure that data contributed by ordinary people outside of traditional organizational boundaries is of suitable quality to be useful for both known and unanticipated purposes. This research examines the impact of different information quality management strategies, and corresponding data collection design choices, on key dimensions of information quality in crowdsourced user-generated content. We conceptualize a contributor-centric information quality management approach focusing on instance-based data collection. We contrast it with the traditional consumer-centric fitness-for-use conceptualization of information quality that emphasizes class-based data collection. We present laboratory and field experiments conducted in a citizen science domain that demonstrate trade-offs between the quality dimensions of accuracy, completeness (including discoveries), and precision between the two information management approaches and their corresponding data collection designs. Specifically, we show that instance-based data collection results in higher accuracy, dataset completeness, and number of discoveries, but this comes at the expense of lower precision. We further validate the practical value of the instance-based approach by conducting an applicability check with potential data consumers (scientists, in our context of citizen science). In a follow-up study, we show, using human experts and supervised machine learning techniques, that substantial precision gains on instance-based data can be achieved with post-processing. We conclude by discussing the benefits and limitations of different information quality and data collection design choices for information quality in crowdsourced user-generated content.;machine learning
Understanding the Elephant: The Discourse Approach to Boundary Identification and Corpus Construction for Theory Review Articles;https://aisel.aisnet.org/jais/vol20/iss7/15;"The goal of a review article is to present the current state of knowledge in a research area. Two important initial steps in writing a review article are boundary identification (identifying a body of potentially relevant past research) and corpus construction (selecting research manuscripts to include in the review). We present a theory-as-discourse approach, which (1) creates a theory ecosystem of potentially relevant prior research using a citation-network approach to boundary identification; and (2) identifies manuscripts for consideration using machine learning or random selection. We demonstrate an instantiation of the theory as discourse approach through a proof-of-concept, which we call the automated detection of implicit theory (ADIT) technique. ADIT improves performance over the conventional approach as practiced in past technology acceptance model reviews (i.e., keyword search, sometimes manual citation chaining); it identifies a set of research manuscripts that is more comprehensive and at least as precise. Our analysis shows that the conventional approach failed to identify a majority of past research. Like the three blind men examining the elephant, the conventional approach distorts the totality of the phenomenon. ADIT also enables researchers to statistically estimate the number of relevant manuscripts that were excluded from the resulting review article, thus enabling an assessment of the review article’s representativeness.";machine learning
An Agent-Based Collaborative Approach to Graphing Causal Maps for Situation Formulation;https://aisel.aisnet.org/jais/vol10/iss3/3/;We provide a background discussion of group support systems (GSS) research into aiding strategic management processes. GSS support for strategic management has been primarily focused on qualitative analysis and the communication processes surrounding strategic planning. While fully developed in common decision-support systems, powerful simulation modeling and quantitative analytical tools have been difficult to integrate into GSS system configurations because they require increased cognitive load and expert modeling support, a central problem now addressed by collaboration engineering. A conceptual and functional bridge is needed to integrate the qualitative and quantitative approaches, reduce cognitive load, and provide modeling support that does not require experts. Acar’s analytical causal mapping is introduced as a structured method for situational formulation and analysis of unstructured strategic problems. This form of causal mapping includes specific processes and analytical approaches offering cognitive modeling support for problem formulation. Its computational capabilities provide support for Systems Thinking approaches in a system easy to learn and use. Using the methodological template of the design science paradigm, we contribute a prototype system for the development and simulation of causal maps that uses RePast 2.0, a Java agent-based modeling (ABM) and simulation library.;machine learning
Regulating Cryptocurrencies: A Supervised Machine Learning Approach to De-Anonymizing the Bitcoin Blockchain;https://doi.org/10.1080/07421222.2018.1550550;Bitcoin is a cryptocurrency whose transactions are recorded on a distributed, openly accessible ledger. On the Bitcoin Blockchain, an owning entity’s real-world identity is hidden behind a pseudonym, a so-called address. Therefore, Bitcoin is widely assumed to provide a high degree of anonymity, which is a driver for its frequent use for illicit activities. This paper presents a novel approach for de-anonymizing the Bitcoin Blockchain by using Supervised Machine Learning to predict the type of yet-unidentified entities. We utilized a sample of 957 entities (with ?385 million transactions), whose identity and type had been revealed, as training set data and built classifiers differentiating among 12 categories. Our main finding is that we can indeed predict the type of a yet-unidentified entity. Using the Gradient Boosting algorithm with default parameters, we achieve a mean cross-validation accuracy of 80.42% and F1-score of ?79.64%. We show two examples, one where we predict on a set of 22 clusters that are suspected to be related to cybercriminal activities, and another where we classify 153,293 clusters to provide an estimation of the activity on the Bitcoin ecosystem. We discuss the potential applications of our method for organizational regulation and compliance, societal implications, outline study limitations, and propose future research directions. A prototype implementation of our method for organizational use is included in the appendix.;machine learning
Organizational Learning in the Rise of Machine Learning;https://core.ac.uk/download/pdf/301385554.pdf;Organizational learning (OL) is associated with experience and knowledge in an organization. Information Technology (IT) enables the creation, dissemination, and use of knowledge, and as such, plays an important role in an organization’s learning process. This role has inspired a large body of literature studying the link between OL and IT and the relation between IT and knowledge exploration and exploitation. The recent rise of Machine Learning (ML) with its Deep Learning (DL) capabilities has nevertheless brought about new ways of creating, retaining, and transferring knowledge. I argue that the learning occurring within the machine plays a role in the learning occurring within the organization, calling for revisiting OL in light of this disruptive IT. In this paper, I focus on three different ways in which the machine achieves its learning, namely supervised, unsupervised, and reinforcement learning, and advance propositions on how each impacts OL differently.;machine learning
Challenges in the Deployment and Operation of Machine Learning in Practice;https://publikationen.bibliothek.kit.edu/1000095028;Machine learning has recently emerged as a powerful technique to increase operational efficiency or to develop new value propositions. However, the translation of a prediction algorithm into an operationally usable machine learning model is a time-consuming and in various ways challenging task. In this work, we target to systematically elicit the challenges in deployment and operation to enable broader practical dissemination of machine learning applications. To this end, we first identify relevant challenges with a structured literature analysis. Subsequently, we conduct an interview study with machine learning practitioners across various industries, perform a qualitative content analysis, and identify challenges organized along three distinct categories as well as six overarching clusters. Eventually, results from both literature and interviews are evaluated with a comparative analysis. Key issues identified include auto- mated strategies for data drift detection and handling, standardization of machine learning infrastructure, and appropriate communication and expectation management.;machine learning
ONEBANKASSURE: CUSTOMER INTIMACY THROUGH MACHINE LEARNING;https://cisr.mit.edu/publication/MIT_CISRwp427_OneBankAssure_BeathTarafdarRoss;OneBankAssure [a pseudonym] sought to delight its customers by delivering new digital value propositions that exploited new data science and machine learning disciplines. The company created a new organizational unit focusing on these disciplines and funded many new use cases that leveraged them. This case describes the practices of this new unit and recounts the story of the development of one of the company’s successful machine learning use cases. To create fertile ground for the use of machine learning capabilities, the company was also making significant investments in training and culture change, so as to become a more data-driven and evidence-based organization.;machine learning
High-performance detection of epilepsy in seizure-free EEG recordings: A novel machine learning approach using very specific epileptic EEG sub-bands;https://aisel.aisnet.org/icis2019/is_health/is_health/16/;We applied machine learning to diagnose epilepsy based on the fine-graded spectral analysis of seizure-free (resting state) EEG recordings. Despite using unspecific agglomerated EEG spectra, our fine-graded spectral analysis specifically identified the two EEG resting state sub-bands differentiating healthy people from epileptics (1.5-2 Hz and 11-12.5 Hz). The rigorous evaluation of completely unseen data of 100 EEG recordings (50 belonging to epileptics and the other 50 to healthy people) shows that the approach works successfully, achieving an outstanding accuracy of 99 percent, which significantly outperforms the current benchmark of 70% to 95% by a panel of up to three experienced neurologists. Our epilepsy diagnosis classifier can be implemented in modern EEG analysis devices, especially in intensive care units where early diagnosis and appropriate treatment are decisive in life and death scenarios and where physicians’ error rates are particularly high. Our approach is accurate, robust, fast, and cost-efficient and substantially contributes to Information Systems research in healthcare. The approach is also of high practical and theoretical relevance.;machine learning
Towards Deep Learning Interpretability: A Topic Modeling Approach;https://aisel.aisnet.org/icis2019/data_science/data_science/26/;The recent development of deep learning has achieved the state-of-the-art performance in various machine learning tasks. The IS research community has started to leveraged deep learning-based text mining for analyzing textual documents. The lack of interpretability is endemic among the state-of-the-art deep learning models, constraining model improvement, limiting additional insights, and prohibiting adoption. In this study, we propose a novel text mining research framework, Neural Topic Embedding, capable of extracting useful and interpretable representations of texts through deep neural networks. Specifically, we leverage topic modeling to enrich deep learning data representations with meaning. To demonstrate the effectiveness of our proposed framework, we conducted a preliminary evaluation experiment on a testbed of fake review detection and our interpretable representations improves the state-of-the-art by almost 8 percent as measured by F1 score. Our study contributes to the IS community by opening the gate for future adoption of the state-of-the-art deep learning methods.;machine learning
Machine Learning Approach for Foot-side Classification using a Single Wearable Sensor;https://www.researchgate.net/publication/338344695_Machine_Learning_Approach_for_Foot-side_Classification_using_a_Single_Wearable_Sensor;Gait analysis is a common technique used to identify problems related to movement and posture in people with injuries, and foot-side detection is one of its important challenges. As many commercial sensors only provide limited information and traditional lab-based gait analysis is expensive, the aim of this study is to discriminate between left and right foot steps based on acceleration data from a single chest-worn accelerometer. To achieve this goal, an experimental study was conducted with 25 participants wearing an accelerometer on their chest and walking in a static environment. Several machine learning (ML) classifiers were trained to detect a foot-side from collected acceleration data. All machine learning classifiers achieved high classification accuracy, with Random Forest providing the best results. This result shows that ML-based foot-side classification using a single sensor is achievable and can contribute to develop an efficient health monitoring system to track lower limb’s problems.;machine learning
Finding the Unicorn: Predicting Early Stage Startup Success Through a Hybrid Intelligence Method;http://dx.doi.org/10.2139/ssrn.3159123;Artificial intelligence is an emerging topic and will soon be able to perform decisions better than humans. In more complex and creative contexts such as innovation, however, the question remains whether machines are superior to humans. Machines fail in two kinds of situations: processing and interpreting “soft” information (information that cannot be quantified) and making predictions in “unknowable risk” situations of extreme uncertainty. In such situations, the machine does not have representative information for a certain outcome. Thereby, humans are still the “gold standard” for assessing “soft” signals and make use intuition. To predict the success of startups, we, thus, combine the complementary capabilities of humans and machines in a Hybrid Intelligence method. To reach our aim, we follow a design science research approach to develop a Hybrid Intelligence method that combines the strength of both machine and collective intelligence to demonstrate its utility for predictions under extreme uncertainty.;machine learning
Deep Recurrent Neural Networks for Mortality Prediction in Intensive Care using Clinical Time Series at Multiple Resolutions;https://www.researchgate.net/publication/343944643_Deep_Recurrent_Neural_Networks_for_Mortality_Prediction_in_Intensive_Care_using_Clinical_Time_Series_at_Multiple_Resolutions;Mortality models in Intensive Care Units (ICU) are important for clinical decision support tasks such as identifying high-risk patients and prioritizing their care. Previous mortality models have used predictive variables mainly from Electronic Medical Records (EMR) where each patient observation can be represented as a sparse multivariate time series. Bedside monitors are another common data source in ICUs containing highresolution time series, which have not been explored in combination with EMR data for mortality modelling. We take the first step towards building such a model. Specialized techniques developed for sparse time series cannot be used to model multiple time series at different resolutions. To address this problem, we develop MTS-RNN, a new deep recurrent neural network architecture. Our preliminary experiments on real clinical data show that MTS-RNN outperforms state-of-the-art mortality models in predictive accuracy, highlighting the importance of using clinical time series at multiple resolutions for ICU mortality prediction.;machine learning
Augmented Intelligence for Quality Control of Manual Assembly Processes using Industrial Wearable Systems;https://aisel.aisnet.org/icis2019/mobile_iot/mobile_iot/9/;Empowered by machine learning and artificial intelligence innovations, IoT devices have become a leading driver of digital transformation. A promising approach are augmented intelligence solutions which seek to enhance human performance in complex tasks. However, there are no turn-key solutions for developing and implementing such systems. One possible avenue is to complement multi-purpose hardware with flexible AI solutions which are adapted to a given task. We illustrate the bottom-up development of a machine learning backend for an augmented intelligence system in the manufacturing sector. A wearable device equipped with highly sensitive sensors is paired with a deep convolutional neural network to monitor connector systems assembly processes in real-time. Our initial study yields promising results in an experimental environment. While this establishes the feasibility of the suggested approach, further evaluations in more complex test cases and ultimately, in a real-world assembly process have to be performed.;machine learning
Decision Support for Negotiation Protocol Selection: A Machine Learning Approach Based on Artificial Neural Networks;http://edoc.sub.uni-hamburg.de/hsu/volltexte/2014/3041/;Decision making in operational planning is increasingly affected by conflicting interests of different stakeholders such as subcontractors, customers, or strategic partners. Addressing this, automated negotiation is a well-suited mechanism to mediate between stakeholders and search for jointly beneficial agreements. However, the outcome of a negotiation is strongly dependent on the applied negotiation protocol defining the rules of encounter. Although protocol design is well discussed in literature, the question on which protocol should be selected for a given scenario is little regarded so far. Since negotiation problems and protocols are very diverse, the protocol choice itself is a challenging task. In this study, we propose a decision support system for negotiation protocol selection (DSS-NPS) that is based on a machine learning approach – an artificial neural network (ANN). Besides presenting and discussing the system, we, furthermore, evaluate the design artifact in elaborate computational experiments that take place in an intercompany machine scheduling environment. Our findings indicate that the proposed decision support system is able to improve the outcome of negotiations by finding adequate protocols dynamically on the basis of the underlying negotiation problem characteristics.;machine learning
Artificial Intelligence and Drug Innovation;https://core.ac.uk/download/pdf/301383955.pdf;We study how artificial intelligence (AI) can influence the drug development process in the global pharmaceutical industry. Despite considerable effort made in developing drugs, pharmaceutical firms experience declines in novelty for drugs they produced. As AI becomes an important general purpose technology (GPT), it could be used to address some known challenges in the drug development process. Using two large-scale datasets that contain detailed historical records of global drug development and patents, we identify AI-related patents to approximate firms’ AI capabilities and construct a relatively new similarity-based metric to measure drug novelty based on their chemical structure. We find that AI can primarily affect the earliest stage in drug discovery when tasks are heavily dependent on automatic data processing and reasoning. However, it may not necessarily help with the more expensive and risky clinical trial stages that require substantial human engagements and interventions. Additionally, AI can facilitate the development for drugs at the medium level of chemical novelty more than at the extreme ends of the spectrum. Our study sheds light on the understanding of the roles and limitations modern technology can have on drug development, one of the most complex innovation processes in the world.;machine learning
GOOD, BAD, OR BOTH? MEASUREMENT OF PHYSICIAN’S AMBIVALENT ATTITUDES TOWARDS AI;https://aisel.aisnet.org/ecis2019_rp/115/;Artificial intelligence is currently one of the most controversial discussed technologies across various work domains. In healthcare, AI fosters widespread positive beliefs of substantially increasing the quality of care, yet evoking physicians’ fears of being marginalized or replaced. The described controversy leads to ambivalent attitudes, as physicians hold both strong positive and negative evaluations at the same time. However, current research in information systems has not been able to measure ambivalence because uni-polar attitude scales cannot assess this construct. Additionally, it is unclear whether ambivalence has positive or negative consequences and how it is related to resistance to change. In the context of AI in healthcare, we conducted a survey study (n=74) to measure context-specific attitudes and attitude ambivalence of physicians. We distinguish between two states of ambivalence and show that physicians who experience an inner conflict (Felt Ambivalence) from conflicting attitudes (Potential Ambivalence) develop resistance to change. Moreover, including ambivalence into a regression model explains more variance than uni-polar attitudes alone. With this research, we show that ambivalent attitudes can be measured in the context of technological change. Additionally, we explore how context-specific attitudes towards AI in healthcare drive physicians’ ambivalence towards it.;machine learning
A Survey of the Application of Machine Learning in Decision Support Systems;https://aisel.aisnet.org/ecis2015_cr/133/;Machine learning is a useful technology for decision support systems and assumes greater importance in research and practice. Whilst much of the work focuses technical implementations and the adaption of machine learning algorithms to application domains, the factors of machine learning design affecting the usefulness of decision support are still understudied. To enhance the understanding of machine learning and its use in decision support systems, we report the results of our content analysis of design-oriented research published between 1994 and 2013 in major Information Systems outlets. The findings suggest that the usefulness of machine learning for supporting decision-makers is dependent on the task, the phase of decision-making, and the applied technologies. We also report about the advantages and limitations of prior research, the applied evaluation methods and implications for future decision support research. Our findings suggest that future decision support research should shed more light on organizational and people-related evaluation criteria.;machine learning
TOWARDS AN INTEGRATIVE THEORETICAL FRAMEWORK OF INTERACTIVE MACHINE LEARNING SYSTEMS;https://aisel.aisnet.org/ecis2019_rp/172/;Interactive machine learning (IML) is a learning process in which a user interacts with a system to iteratively define and optimise a model. Although recent years have illustrated the proliferation of IML systems in the fields of Human-Computer Interaction (HCI), Information Systems (IS), and Computer Science (CS), current research results are scattered leading to a lack of integration of existing work on IML. Furthermore, due to diverging functionalities and purposes IML systems can refer to, an uncertainty exists regarding the underlying distinct capabilities that constitute this class of systems. By reviewing extensive IML literature, this paper suggests an integrative theoretical framework for IML systems to address these current impediments. Reviewing 2,879 studies in leading journals and conferences during the years 1966-2018, we found an extensive range of applications areas that have implemented IML systems and the necessity to standardise the evaluation of those systems. Our framework offers an essential step to provide a theoretical foundation to integrate concepts and findings across different fields of research. The main contribution of this paper is organising and structuring the body of knowledge in IML for the advancement of the field. Furthermore, we suggest three opportunities for future IML research. From a practical point of view, our integrative theoretical framework can serve as a reference guide to inform the design and implementation of IML systems.;machine learning
Intelligent Agents as a Modeling Paradigm;https://aisel.aisnet.org/icis2005/15/;Intelligent software agents have been used in many applications because they provide useful integrated features that are not available in “traditional” types of software (e.g., abilities to sense the environment, reason, and interact with other agents). Although the usefulness of agents is in having such capabilities, methods and tools for developing them have focused on practical physical representation rather than accurate conceptualizations of these functions. However, intelligent agents should closely mimic aspects of the environment in which they operate. In the physical sciences, a conceptual model of a problem can lead to better theories and explanations about the area. Therefore, we ask, can an intelligent agent conceptual framework, properly defined, be used to model complex interactions in various social science disciplines? The constructs used in the implementation of intelligent agents may not be appropriate at the conceptual level, as they refer to software concepts rather than to application domain concepts. We propose to use a combina- tion of the systems approach and Bunge’s ontology as adapted to information systems, to guide us in defining intelligent agent concepts. The systems approach will be used to define the components of the intelligent agents and ontology will be used to understand the configurations and interrelationships between the components. We will then provide a graphical representation of these concepts for modeling purposes. As a proof of concept for the proposed conceptual model, we applied it to a marketing problem and imple- mented it in an agent-based programming environment. Using the conceptual model, the user was able to quickly visualize the complex interactions of the agents. The use of the conceptual representation even sparked an investigation of previously neglected causal factors which led to a better understanding of the problem. Therefore, our intelligent agent framework can graphically model phenomena in the social sciences. This work also provides a theoretically driven concept of intelligent agent components and a definition of the inter- relationships between these concepts. Further research avenues are also discussed.;machine learning
Making Business Predictions by Combining Human and Machine Intelligence in Prediction Markets;https://www.researchgate.net/publication/221599614_Making_Business_Predictions_by_Combining_Human_and_Machine_Intelligence_in_Prediction_Markets;Computers can use vast amounts of data to make predictions that are often more accurate than those by human experts. Yet, humans are more adept at processing unstructured information and at recognizing unusual circumstances and their consequences. Can we combine predictions from humans and machines to get predictions that are better than either could do alone? We used prediction markets to combine predictions from groups of people and artificial intelligence agents. We found that the combined predictions were both more accurate and more robust than those made by groups of only people or only machines. This combined approach may be especially useful in situations where patterns are difficult to discern, where data are difficult to codify, or where sudden changes occur unexpectedly;machine learning
Transfer Learning in Dynamic Business Environments: An Application in Earnings Forecast for Public Firms;https://aisel.aisnet.org/icis2019/data_science/data_science/6/;In dynamic business environments, the underlying true data pattern changes rapidly. Machine learning models built upon historical data may not be responsive to the changes. A simple solution is to re-train a machine learning model using the re-collected current data. However, current data are often scarce. Therefore, it would be optimal to adapt the machine learning model built on historical data to the current period. In this study, we propose a two-step transfer learning method for enhancing machine learning in dynamic data environments. Our insight is that, by comparing current data and historical data, we gain information on the change of data environments, which guides the training of machine learning using historical and current data sets simultaneously. In this research-in-progress, we evaluate our method and an existing state-of-art algorithm in the earnings prediction tasks. Preliminary results show the effectiveness of transfer learning in dynamic business environments.;machine learning
Effective Sentiment Analysis of Corporate Financial Reports;https://aisel.aisnet.org/icis2013/proceedings/ResearchInProgress/55/;Sentiment analysis is widely adopted in studying various important topics in business intelligence. Though many studies reported interesting results by using machine learning, the lack of theoretic analysis and the shortage of practical guidance are hurdles of theory development. Besides, due to the difficulty in labeling data, the effectiveness of sentiment analysis with only labelled data needs to be questioned. In this paper, we drew on statistical learning theory to perform extensive theoretic analysis in sentiment analysis by using real corporate financial reports. We investigated when and why machine learning methods provide preferred performance under the guidance of the theory. We also provided practical suggestions in applying machine learning methods for both researchers and practitioners. In addition, we utilized the cheap and ubiquitous unlabeled data to further improve the sentiment analysis performance. This has the potential to largely reduce the manual data labeling work and to scale up the experiments.;machine learning
Personalized explanation in machine learning: A conceptualization;https://doi.org/10.48550/arXiv.1901.00770;Explanation in machine learning and related fields such as artificial intelligence aims at making machine learning models and their decisions understandable to humans. Existing work suggests that personalizing explanations might help to improve understandability. In this work, we derive a conceptualization of personalized explanation by defining and structuring the problem based on prior work on machine learning explanation, personalization (in machine learning) and concepts and techniques from other domains such as privacy and knowledge elicitation. We perform a categorization of explainee data used in the process of personalization as well as describing means to collect this data. We also identify three key explanation properties that are amendable to personalization: complexity, decision information and presentation. We also enhance existing work on explanation by introducing additional desiderata and measures to quantify the quality of personalized explanations.;machine learning
Advancing Recommendations on Two-Sided Platforms: A Machine Learning Approach to Context-Aware Profiling;https://aisel.aisnet.org/icis2019/data_science/data_science/21/;Digitally enabled two-sided platforms rely on mediating different actors to evoke transactions. Here, the core value-generating mechanisms of these platforms relate to the recommendations that persuade users to make future transactions, thereby driving sales, customer satisfaction, efficiency, and trust. To generate effective recommendations, accurate user profiling is fundamental. Ubiquitous computing provides valuable data to enhance user profiling by uncovering behavioral patterns of individual users. Specifically, through machine learning methods, recommendation systems are able to understand users better by considering both past individual behaviors and their respective contexts. However, state-of-the-art recommendation systems rely either on collaborative or content-based approaches, thereby neglecting a user’s time-varying contexts and the dynamics that influence these contexts. We address this shortcoming by developing a context-aware and user-specific hybrid recommendation system using transfer-learning techniques based on (recurrent) neural networks. Evaluating our approach on Expedia’s hotel booking data, we demonstrate its enhanced performance compared to common recommendation approaches.;machine learning
Towards a Future Reallocation of Work between Humans and Machines – Taxonomy of Tasks and Interaction Types in the Context of Machine Learning;https://aisel.aisnet.org/icis2017/TransformingSociety/Presentations/14/;In today’s race for competitive advantages, more and more companies implement innovations in artificial intelligence and machine learning (ML). Although these machines take over tasks that have been executed by humans, they will not make human workforce obsolete. To leverage the potentials of ML, collaboration between humans and machines is necessary. Before collaboration processes can be developed, a classification of tasks in the field of ML is needed. Therefore, we present a taxonomy for the classification of tasks due to their complexity and the type of interaction. To derive insights about typical tasks and task-complexity, we conducted a literature review as well as a focus group workshop. We identified three levels of task-complexity and three types of interactions. Connecting them reveals three generic types of tasks. We provide prescriptive knowledge inherent in the task/interaction-taxonomy.;machine learning
A Customized and Interpretable Deep Neural Network for High-Dimensional Business Data - Evidence from an E-Commerce Application;https://aisel.aisnet.org/icis2017/DataScience/Presentations/13/;Extracting actionable information from complex data is a key challenge for business analytics researchers (Hedgebeth, 2007). This is particularly difficult for high-dimensional datasets, to which an increasing number of businesses have access (Martens et al., 2016). In this study, we develop a customized neural network for extracting interpretable features from very high-dimensional datasets. These features can be interpreted both at an aggregated as well as a very fine-grained level. Interpreting non-linear interactions is no more difficult than interpreting a linear regression. We apply the algorithm to a dataset related to product returns in online retail which contains a total of 3,637,654 transactions and 13,533 dimensions. Comparing 75 different models, we demonstrate that, in addition to being interpretable, our algorithm yields higher predictive accuracy than extant methods. The approach is sufficiently abstract to be applicable to a wide variety of business analytics datasets.;machine learning
Hiring Algorithms: An Ethnography of Fairness in Practice;https://aisel.aisnet.org/icis2019/future_of_work/future_work/6/;While increasing attention in society is given to the role of AI in affording and threatening ethical values, such as fairness, little is known about how ethical values and AI are played out in organizations. Building on an ethnographic in-depth study of a large multinational company that recently implemented AI to enable a fair recruitment process, we show that AI brings to the fore the role of fairness in decision-making in several ways. We reveal that the development and use of AI does not necessarily improve nor degrade ethical values, but instead shapes what comes to be understood as ethical in the first place. We extend the conversations on AI by showing that it may not be enough to focus on changes in work practices, occupational boundaries, and power relations, but that research should take into account the role of AI in shaping what we consider ethical.;machine learning
The Impact of Deep Learning on Organizational Agility;https://aisel.aisnet.org/icis2019/governance_is/governance_is/26/;Artificial intelligence advances business model, strategizes competitive resources, and impacts on organizational agility. Deep learning as a subset of AI brings changes in different aspects that substantially influences organizational capabilities. We argue that deep learning enables new conceptualization of organizational agility. We will conduct a case study in a leading Chinese FinTech company to inductively ground these impacts.;machine learning
Does AI-based Credit Scoring Improve Financial Inclusion? Evidence from Online Payday Lending;https://aisel.aisnet.org/icis2019/blockchain_fintech/blockchain_fintech/20/;Artificial intelligence (AI) has become ubiquitous in the consumer finance industry. One of the major AI applications in this industry is AI-based credit scoring models. We investigate whether AI applications improve financial inclusion, as measured by three seemingly contradictory metrics, i.e. approval rate, default rate, and false rejection rate. We cooperate with an AI solution provider whose AI-based credit scoring models are widely used by online lenders in China. Using data obtained from these online lenders, we find that AI-based credit scoring models increase approval rate and reduce default rate simultaneously, which enhances both the magnitude and the quality of financial inclusion. AI-based credit scoring models also tend to reduce false rejection rate, suggesting that they can help provide access to capital to a previously underserved population. We plan to collect more data and conduct additional analyses in the future to enrich our current findings and explore for underlying mechanisms.;machine learning
Displaced or Augmented? How does Artificial Intelligence Affect Our Jobs: Evidence from LinkedIn;https://aisel.aisnet.org/icis2019/economics_is/economics_is/28/;With the rapid advances of artificial intelligence (AI), increasingly more job tasks can be automated. Despite the AI hype, we know little about the extent to which AI may destroy or augment the career of different occupations of professionals. Although most existing literature focused on creating AI automation scores for each occupation, AI may automate non-critical tasks for many occupations which indirectly increases the productivity and value creation of jobs. Therefore, we develop a novel method to estimate the AI automation scores for core and supplemental work activities of all major occupations and analyze how employees’ human capital characteristics may lead to different results: being augmented or displaced by AI. Particularly, skills accumulated from prior work experiences and excellent educational background can reduce the automation risks. Additionally, professionals with major in computing, law, and medicine are more likely to be augmented since only their supplemental work activities may be automated.;machine learning
EXPLAINING CUSTOMER ACTIVATION WITH DEEP ATTENTION MODELS;https://aisel.aisnet.org/ecis2019_rp/151/;Effectively informing consumers is a big challenge for financial service providers. Triggering involvement in the personal situation of the client is a result of sending relevant information at the right time. While general machine learning techniques are able to accurately predict the behavior of consumers, they tend to lack interpretability. This is a problem since interpretation aims at producing the information a communication department requires to be able to trigger involvement. In this paper we provide a solution for predicting and explaining customer activation as result of a series of events, by means of deep learning and attention models. The proposed solution is applied to data concerning the activity of pension fund participants and compared to standard machine learning techniques on both accuracy and interpretability. We conclude that the attention based model is as accurate as top tier machine learning algorithms in predicting customer activation, while being able to extract the key events in the communication with a single customer. This results in the ability to help understand the needs of customers on a personal level and to construct an individual marketing strategy for each customer.;machine learning
Leveraging Deep-learning and Field Experiment Response Heterogeneity to Enhance Customer Targeting Effectiveness;https://aisel.aisnet.org/icis2019/data_science/data_science/28/;Firms seek to better understand heterogeneity in the customer response to marketing campaigns, which can boost customer targeting effectiveness. Motivated by the success of modern machine learning techniques, this paper presents a framework that leverages deep-learning algorithms and field experiment response heterogeneity to enhance customer targeting effectiveness. We recommend firms run a pilot randomized experiment and use the data to train various deep-learning models. By incorporating recurrent neural nets and deep perceptron nets, our optimal deep-learning model can capture both temporal and network effects in the purchase history, after addressing the common issues in most predictive models such as imbalanced training, data sparsity, temporality, and scalability. We then apply the learned optimal model to identify customer targets from the large amount of remaining customers with the highest predicted purchase probabilities. Our application with a large department store on a total of 2.8 million customers supports that optimal deep-learning models can identify higher-value customer targets and lead to better sales performance of marketing campaigns, compared to industry common practices of targeting by past purchase frequency or spending amount. We demonstrate that companies may achieve sub-optimal customer targeting not because they offer inferior campaign incentives, but because they leverage worse targeting rules and select low-value customer targets. The results inform managers that beyond gauging the causal impact of marketing interventions, data from field experiments can also be leveraged to identify high-value customer targets. Overall, deep-learning algorithms can be integrated with field experiment response heterogeneity to improve the effectiveness of targeted campaigns.;machine learning
A human-centric perspective exploring the readiness towards smart warehousing: The case of a large retail distribution warehouse;https://doi.org/10.1016/j.ijinfomgt.2018.11.008;The explosive rise in technologies has revolutionised the way in which business operate, consumers buy, and the pace at which these activities take place. These advancements continue to have profound impact on business processes across the entire organisation. As such, Logistics and Supply Chain Management (LSCM) are also leveraging benefits from digitisation, allowing organisations to increase efficiency and productivity, whilst also providing greater transparency and accuracy in the movement of goods. While the warehouse is a key component within LSCM, warehousing research remains an understudied area within overall supply chain research, accounting for only a fraction of the overall research within this field. However, of the extant warehouse research, attention has largely been placed on warehouse design, performance and technology use, yet overlooking the determinants of Artificial Intelligence (AI) adoption within warehouses. Accordingly, through proposing an extension of the Technology–Organisation–Environment (TOE) framework, this research explores the barriers and opportunities of AI within the warehouse of a major retailer. The findings for this qualitative study reveal AI challenges resulting from a shortage of both skill and mind-set of operational management, while also uncovering the opportunities presented through existing IT infrastructure and pre-existing AI exposure of management.;machine learning
Applying artificial intelligence technique to predict knowledge hiding behavior;https://doi.org/10.1016/j.ijinfomgt.2019.02.006;Drawing on psychological ownership and social exchange theories, this study suggests theoretical arguments and empirical evidence for understanding employee reactions to distributive, procedural, and interactional (in)justice — three crucial bases of employees’ feelings of social self-worth. Utilizing field data and artificial intelligence technique, this paper reveals that distributive, procedural, and interactional (in)justice contribute to higher levels of knowledge hiding behavior among employees and that this impact is non-linear (asymmetric). By reuniting the discourses of organizational justice and knowledge management, this study indicates that feelings of psychological ownership of knowledge and the degree of social interaction are mechanisms that work with organizational (in)justice to influence knowledge hiding behavior. The current research may inform contemporary theories of business research and provide normative guidance for managers.;machine learning
Disaster City Digital Twin: A vision for integrating artificial and human intelligence for disaster management;https://doi.org/10.1016/j.ijinfomgt.2019.102049;"This paper presents a vision for a Disaster City Digital Twin paradigm that can: (i) enable interdisciplinary convergence in the field of crisis informatics and information and communication technology (ICT) in disaster management; (ii) integrate artificial intelligence (AI) algorithms and approaches to improve situation assessment, decision making, and coordination among various stakeholders; and (iii) enable increased visibility into network dynamics of complex disaster management and humanitarian actions. The number of humanitarian relief actions is growing due to the increased frequency of natural and man-made crises. Various streams of research across different disciplines have focused on ICT and AI solutions for enhancing disaster management processes. However, most of the existing research is fragmented without a common vision towards a converging paradigm. Recognizing this, this paper presents the Disaster City Digital Twin as a unifying paradigm. The four main components of the proposed Digital Twin paradigm include: multi-data sensing for data collection, data integration and analytics, multi-actor game-theoretic decision making, and dynamic network analysis. For each component, the current state of the art related to AI methods and approaches are examined and gaps are identified.";machine learning
On the training of a neural network for online path planning with offline path planning algorithms;https://doi.org/10.1016/j.ijinfomgt.2020.102142;One of the challenges in path planning for an automated vehicle is uncertainty in the operational environment of the vehicle, demanding a quick but sophisticated control of the vehicle online. To address this online path planning issue, neural networks, which can derive a heading for an operating vehicle in a given situation, have been actively studied, demonstrating their satisfactory performance. However, the study on the training path data, which specifies the desired output of a neural network and in turn influences the behavior of the neural network, has been neglected in the literature. Motivated by this fact, in this paper, we first generate different training path data sets applying two different offline path planning algorithms and evaluate the performance of a neural network as an online path planner depending on the training data under a simulation environment. We further investigate the properties of the training data that make a neural network more reliable for online path planning.;machine learning
PTZ-Surveillance coverage based on artificial intelligence for smart cities;https://doi.org/10.1016/j.ijinfomgt.2019.04.017;"Surveillance cameras have a plethora of usages in newly born cities including smart traffic, healthcare, monitoring, and meeting security needs. One of the most famous new cites is the Egypt's new administration capital “New Cairo”. The new administration capital of Egypt mainly characterizes with the green life style via the ""Green River "". In this paper, a new enhanced Artificial Intelligence (AI) algorithm is introduced for adjusting the orientation of Pan–Tilt–Zoom (PTZ) surveillance cameras in new Cairo. In other words, the new proposed algorithm is used for improving the field of view (FOV) coverage of PTZ cameras network. For validating the proposed algorithm, it is tested on many scenarios with different criterions. After that, the proposed algorithm is applied to adjust the PTZ monitoring cameras in the green river which locates on new administrative capital as an equivalent to the river Nile. In addition, it compared with several other AI algorithms through the appropriate statistical analysis. The overall experimental results indicate the prosperity of the proposed algorithm for increasing the coverage of the PTZ surveillance system.";machine learning
Put your money where your mouth is: Using deep learning to identify consumer tribes from word usage;https://doi.org/10.1016/j.ijinfomgt.2019.03.011;Internet and social media offer firms novel ways of managing their marketing strategy and gain competitive advantage. The groups of users expressing themselves on the Internet about a particular topic, product, or brand are frequently called a virtual tribe or E-tribe. However, there are no automatic tools for identifying and studying the characteristics of these virtual tribes. Towards this aim, this paper presents Tribefinder, a system to reveal Twitter users’ tribal affiliations, by analyzing their tweets and language use. To show the potential of this instrument, we provide an example considering three specific tribal macro-categories: alternative realities, lifestyle, and recreation. In addition, we discuss the different characteristics of each identified tribe, in terms of use of language and social interaction metrics. Tribefinder illustrates the importance of adopting a new lens for studying virtual tribes, which is crucial for firms to properly design their marketing strategy, and for scholars to extend prior marketing research.;machine learning
Setting the future of digital and social media marketing research: Perspectives and research propositions;https://doi.org/10.1016/j.ijinfomgt.2020.102168;The use of the internet and social media have changed consumer behavior and the ways in which companies conduct their business. Social and digital marketing offers significant opportunities to organizations through lower costs, improved brand awareness and increased sales. However, significant challenges exist from negative electronic word-of-mouth as well as intrusive and irritating online brand presence. This article brings together the collective insight from several leading experts on issues relating to digital and social media marketing. The experts’ perspectives offer a detailed narrative on key aspects of this important topic as well as perspectives on more specific issues including artificial intelligence, augmented reality marketing, digital content management, mobile marketing and advertising, B2B marketing, electronic word of mouth and ethical issues therein. This research offers a significant and timely contribution to both researchers and practitioners in the form of challenges and opportunities where we highlight the limitations within the current research, outline the research gaps and develop the questions and propositions that can help advance knowledge within the domain of digital and social marketing.;machine learning
FOOL ME ONCE, SHAME ON YOU, FOOL ME TWICE, SHAME ON ME: A TAXONOMY OF ATTACK AND DE-FENSE PATTERNS FOR AI SECURITY;https://aisel.aisnet.org/ecis2020_rp/166/;Advances in the area of AI systems lead to the application of complex deep neural networks (DNN) that outperform other algorithms in critical applications like predictive maintenance, healthcare or autonomous driving. Unfortunately, the properties that render them so successful also lead to vulnerabilities that can make them the subject of adversarial attacks. While these systems try to mimic human behavior when transforming large amounts of data into decision recommendations, they remain black-box models so that humans often fail to detect adversarial behavior patterns in the model training process. Therefore, we derive a taxonomy from an extensive literature review to structure the knowledge of possible attack and defense patterns to create a basis for the analysis and implementation of AI security for scientists and practitioners alike. Furthermore, we use the taxonomy to expose the most common attack pattern and, in addition, we demonstrate the application of the taxonomy by projecting two real-world cases onto the taxonomy space and discuss applicable attack and defense patterns.;machine learning
PRESCRIPTIVE PROCESS ANALYTICS WITH DEEP LEARNING AND EXPLAINABLE ARTIFICIAL INTELLIGENCE;https://aisel.aisnet.org/ecis2020_rp/122/;The proliferation of enterprise information systems allows to capture the digital footprints generated over various user interaction phases. Such transaction data describe the details of the user interactions and the underlying processes on a fine granular level. Building capabilities to analyse the transactional process data is a key success differentiation factor that enables to grasp the user behavior more effectively. In this study, we aim to propose a prescriptive process analytics approach by combining the approaches from the machine learning, process mining and explainable artificial intelligence (XAI) research domains. After examining predictability of the business processes by employing an advanced deep learning approach, this study applies for the first time both in the business process prediction and customer journey analytics research domains an XAI technique, Partial Dependence Plots (PDP), to generate causal explanations. The real-life process data delivered by various information systems of a Dutch autonomous administrative authority were used to investigate the appropriateness of the proposed prescriptive analytics approach. The applied deep learning approach achieves a very good performance with an Area Under ROC Curve of 0.933. The generated explanations with PDP give insights to identify a set of alternative courses-of-actions to prevent the undesired outcomes.;machine learning
The Convergence of Distributed Ledger Technology and Artificial Intelligence: An end-to-end Reference Lending Process for Financial Services;https://www.alexandria.unisg.ch/260329/;Distributed Ledger Technology (DLT) and Artificial Intelligence (AI) represent two potential disruptive technologies at the top of their hype cycle. Subsequently, questions arise what impact these technologies can have on future business models, especially for service-driven industries like the financial sector. While various assumptions in practice indicate a complementary usage of both DLT and AI to generate new value creation potentials, current literature and research remains scarce. To understand possible synergies for financial services, a segregated perspective on DLT or AI alone is not enough. Therefore, the main objective of this paper is to gain first insights how specific elements of these technologies can be mutually implemented and combined for a potential technological convergence on basis of an end-to-end lending reference process. Building upon the existing body of knowledge and based on Design Science Research, an instantiation of the re-designed process has been created in three iterative cycles. The process prototype demonstrates that DLT and AI are complementary technologies and mostly do not compete against each other with a focus on subsequent synergies. Finally, a comparative overview of the impact on the respective sub-processes has been elaborated to conduct principles for the design and development of future distributed-ledger-based AI applications.;machine learning
THE HUMANS BEHIND ARTIFICIAL INTELLIGENCE – AN OPERATIONALISATION OF AI COMPETENCIES;https://aisel.aisnet.org/ecis2020_rp/141/;Despite the importance of artificial intelligence (AI) proficiency as a determinant for AI adoption, there remains a lack of empirical research studying competencies needed to leverage AI effectively. This paper addresses this research gap with a mixed methods approach. First, we conduct a qualitative content analysis of the practical and scientific literature to derive and structure the existing body of knowledge. We subsequently perform a quantitative content analysis of 9,247 job advertisements. We merge the results using a triangulation approach and a) present a comprehensive overview of key technical and managerial competencies essential for implementing and utilising AI on an individual level, b) highlight the demand for AI-related competencies in the three occupational fields Data Science and Engineering, Software Engineering and Development, and Business Development and Sales, and c) underline the need to adapt workforce competencies to a labour market transformation induced by AI.;machine learning
The Impact of Artificial Intelligence on Individual Performance: Exploring the Fit between Task, Data, and Technology;https://aisel.aisnet.org/ecis2020_rp/200/;Artificial intelligence (AI) is increasingly deployed in organizations, allowing information systems (IS) to incorporate self-learning mechanisms. Machine learning (ML) is commonly used as the underlying technology, as it enables IS to derive patters from collected data and perform tasks that were previously reserved for humans. While organizations hope to increase their efficiency and effectivity through adopting AI, the actual linkage between AI use and performance impacts for individuals remains largely overlooked in IS research so far. Therefore, we employ a qualitative research approach to develop a theoretical model for this relationship. In detail, we conduct expert interviews and build on the widely used “task-technology-fit” (TTF) theory. We identify relevant dimensions for the main theory constructs and expand the theory with further components to fit the AI context. Our findings enable future empirical research regarding performance impacts of AI use. Practitioners can use our model to evaluate use cases for AI adoption by considering task, data, and technology characteristics.;machine learning
Actualizing affordances – Story of Indian Information Technology (IT) Industry delivering Artificial Intelligence based solutions;https://aisel.aisnet.org/icis2020/is_development/is_development/3/;Indian IT industry with its long history in delivering quality customized software solutions is gearing up to provide solutions around emerging technologies such as AI, to its clients. In this study, we attempt to explore this transition through the lens of affordances within the IS literature. We base our findings on a four-month ethnography, where one of us worked as a part of a team providing AI solutions in an IT organization situated in Bengaluru, India. Applying Volkoff and Strong’s critical realist framework to the nature of affordances that are being actualized, we compare work practices and processes in this AI project with traditional software development. Given that these two call for different affordances, our in-situ observations parse the actualization of affordances within the organization. Our findings indicate that affordances associated with traditional software development continue to get actualized as work processes even in the context of AI projects.;machine learning
Building Trust in Intelligent Automation: Insights into Structural Assurance Mechanisms for Autonomous Vehicles;https://aisel.aisnet.org/icis2020/iot_smart/iot_smart/7/;Intelligent automation is increasingly taking over tasks that normally require substantial human experience and intuition. However, for individuals to delegate full control to applications like autonomous vehicles (AVs), they need to establish sufficient initial trust in the automation's functionality, reliability and transparency. Manufacturers and external institutions may build users' initial trust by providing structural assurance. Answering calls for a more context-specific, theoretically substantiated investigation of trust in AVs, we investigate how five different forms of structural assurance can be designed and how effective they are in trust-building: Technical, provider, legal, certifier and social protection. Extending previous, survey-based research, we conducted a choice-based conjoint experiment (n = 220). We find that external structural assurance in the form of legal and certifier protection may even outperform manufacturers' trust-building efforts. This is especially the case for some user groups, as a cluster analysis reveals individuals' heterogeneous preferences for structural assurance mechanisms.;machine learning
Combining Blockchain and Artificial Intelligence – Literature Review and State of the Art;https://aisel.aisnet.org/icis2020/blockchain_fintech/blockchain_fintech/6/;Artificial intelligence and blockchain are among the most popular technologies. Combine the two technologies also harbors manifold potentials. For instance, blockchain can help address specific AI-related difficulties such as the black box problem. Vice versa, AI offers opportunities to improve the blockchain’s mining process, or smart contracts. Despite their relevance for companies, these combination solutions have so far received little attention. We undertake a systematic literature review to close this research gap and to provide a first comprehensive overview of this emerging field. We do so by providing a threefold categorization of the different options for connecting the blockchain and AI.;machine learning
How Much AI Do You Require? Decision Factors for Adopting AI Technology;https://aisel.aisnet.org/icis2020/implement_adopt/implement_adopt/10/;Artificial intelligence (AI) based on machine learning technology disrupts how knowledge is gained. Nevertheless, ML’s improved accuracy of prediction comes at the cost of low traceability due to its black-box nature. The field of explainable AI tries to counter this. However, for practical use in IT projects, these two research streams offer only partial advice for AI adoption as the trade-off between accuracy and explainability has not been adequately discussed yet. Thus, we simulate a decision process by implementing three best practice AI-based decision support systems for a high-stake maintenance decision scenario and evaluate the decision and attitude factors using the Analytical Hierarchy Process (AHP) through an expert survey. The combined results indicate that system performance is still the most important factor and that implementation effort and explainability are relatively even factors. Further, we found that systems using similarity-based matching or direct modeling for remaining useful life estimation performed best.;machine learning
"Leveraging ""AI-as-a-Service"" - Antecedents and Consequences of Using Artificial Intelligence Boundary Resources";https://aisel.aisnet.org/icis2020/governance_is/governance_is/6/;"Increased data availability and computing power allows businesses across industries to employ AI technologies in their products and processes. Yet, leveraging AI effectively requires high investments. To reduce these investments, companies can implement AI capabilities directly via boundary resources such as application programming interfaces (APIs). However, research tells us little about antecedents and performance consequences of the use of AI boundary resources. To close this gap, we derive hypotheses from the resource-based view and the relational view of competitive advantages and test these hypotheses on a panel dataset of S&P 500 firms for the years 2010 to 2018. Our results show that firms with high levels of internal AI capabilities are particularly likely to adopt AI boundary resources for process improvements; firms with high external market pressure are positively associated to use AI boundary resources for customer solutions; and the use of AI boundary resources has a positive performance effect.";machine learning
Realizing Digital Innovation from Artificial Intelligence;https://aisel.aisnet.org/icis2020/digital_innovation/digital_innovation/6/;Artificial Intelligence (AI) promises to have a transformational impact on our economy and society. It offers the opportunity for digital innovation, as it can be embedded in products and services. However, although companies are adopting AI, success stories are limited. Similarly, research on how to realize digital innovation from AI is scarce. To analyze the role of AI for digital innovation and how it affects the venture creation process, we conduct an in-depth case study at a heavily-funded medical imaging AI company. Our case study reveals four AI-caused tensions, which a digital venture faces, and we discuss four ways in which it counters them: (1) Managing Over-Expectations of AI, (2) Designing Work Routines for AI, (3) Addressing Opposing User Perceptions of AI, and (4) Integrating Domain Expertise with AI. Ultimately, we hope to contribute to the understanding of digital innovation in the context of AI.;machine learning
The Impact of AI-powered Shelf Monitoring on Product Sales;https://aisel.aisnet.org/icis2020/hci_artintel/hci_artintel/16/#:~:text=We%20find%20that%20AI%2Dpowered,retailers%20rather%20than%20chained%20retailers.;We collaborate with Danone to study how AI-based shelf monitoring helps with manufacturers' shelf management efforts by using data from a field experiment. We find that AI-powered shelf monitoring significantly improves product sales. This effect is only partially persistent in that it diminishes after monitoring is terminated. We further reveal that the positive effect is attributed to independent retailers rather than chained retailers. Since the major difference in shelf monitoring between these two types of retailers is the degree of heterogeneity in shelf space rental contracts, this finding indicates that AI-powered monitoring is better than human monitoring when facing more heterogeneous shelf displays. The finding further suggests the better scalability of AI in coping with more heterogeneous objects. We also interview with the delegates and find a low marginal cost of adopting, which suggests a long-term applicability of AI-powered shelf monitoring to generate value for the manufacturer.;machine learning
The influence of algorithm aversion and anthropomorphic agent design on the acceptance of AI-based job recommendations;https://aisel.aisnet.org/icis2020/is_workplace_fow/is_workplace_fow/4/;Artificial intelligence (AI) offers promising tools to support the job-seeking process by providing automatic and user-centered job recommendations. However, job seekers often hesitate to trust AI-based recommendations in this context given the far-reaching consequences of the importance of the decision for a job on their future career and life. This hesitation is largely driven by a lack of explainability, as underlying algorithms are complex and not clear to the user. Prior research suggests that anthropomorphization (i.e., the attribution of human traits) can increase the acceptance of technology. Therefore, we adapted this concept for AI-based recommender systems and conducted a survey-based study with 120 participants. We find that that using an anthropomorphic design in a recommender system for open positions increases job seekers' acceptance of the underlying system. However, algorithm aversion rises if detailed information on the algorithmic origin is being disclosed.;machine learning
Using Machine Learning to Improve the Sustainability of the Online Review Market;https://aisel.aisnet.org/icis2020/societal_impact/societal_impact/14/;Powered by the Internet, the online review market has grown exponentially, providing a trove of information to customers and business. However, cracks have started to appear around the economic, social and environmental sustainability of online reviews and surrounding processes. The root of these concerns lies in the number of reviews having no informational value. With the aim of improving the sustainability of this market, the present research develops and compares seven machine learning approaches to identify waste in online app reviews. The Random Forest approach shows the best performance with accuracy of 0.94. If such approach were implemented to reduce data waste in 11 app stores, 252,611 kg of CO2, US$ 74,392 and 25,880 person hours could be saved. Having demonstrated its potential for app reviews, the developed approach could be extended to achieve greater savings and improve sustainability across different segments and types of online reviews.;machine learning
Augmenting the algorithm: Emerging human-in-the-loop work configurations;https://doi.org/10.1016/j.jsis.2020.101614;How do configurations of humans and algorithms evolve as firms adopt artificial intelligence (AI) capabilities, and what are the implications for work and organization? We explored these questions through a two-year long case study of an organization in the international maritime trade that introduced automated algorithmic support for data analysis and prediction work. Drawing on a human–machine configuration perspective, we found that humans and the algorithm were configured and reconfigured in multiple ways over time as the organization dealt with the introduction of algorithmic analysis. In contrast to replacing human work, the emergent configurations required new roles and redistribution of extant expertise to augment and improve the accuracy of the algorithm. Our analysis suggests that the new configuration resembled a human-in-the-loop pattern, comprised of both the augmentation work of auditing (i.e. the generation of a ground truth and assessment of the algorithmic output against this) as well as the work of altering the algorithm and the data acquisition architecture. Our research points to the strategic importance of a human-in-the-loop pattern for organizational reflexivity to ensure that the performance of the algorithm meets the organization's requirements and changes in the environment.;machine learning
Human Identification for Activities of Daily Living: A Deep Transfer Learning Approach;https://doi.org/10.1080/07421222.2020.1759961;Sensor-based home Activities of Daily Living (ADLs) monitoring systems have emerged to monitor elderly people’s self-care ability remotely. However, the unobtrusive, privacy-friendly object motion sensor-based systems face challenges such as scarce labeled data and ADL performer confusion in a multi-resident setting. This study adopts the design science paradigm to develop an innovative deep transfer learning framework for human identification (DTL-HID) to address both challenges. A novel convolutional neural network (CNN) is proposed to automatically extract comprehensive temporal and cross-axial motion patterns for the DTL-HID framework. We rigorously evaluate the DTL-HID framework against state-of-the-art benchmarks (e.g., k Nearest Neighbors, Support Vector Machines, and alternative CNN designs). Results demonstrate our proposed DTL-HID framework can identify the ADL performer accurately even on a small amount of labeled data. We demonstrate a case study and discuss how stakeholders can further apply this approach to unobtrusive smart home monitoring for senior citizens. Beyond demonstrating the framework’s practical utility, we discuss two implications of our design principles to mobile analytics and design science research: (1) extracting temporal and axial local dependencies can capture richer information from multi-axial time-series data and (2) transferring knowledge learned on a relevant source domain with sufficient data can improve the performance of the desired task on the target domain with scarce data.;machine learning
Data mining fool’s gold;https://doi.org/10.1177/0268396220915600;The scientific method is based on the rigorous testing of falsifiable conjectures. Data mining, in contrast, puts data before theory by searching for statistical patterns without being constrained by prespecified hypotheses. Artificial intelligence and machine learning systems, for example, often rely on data-mining algorithms to construct models with little or no human guidance. However, a plethora of patterns are inevitable in large data sets, and computer algorithms have no effective way of assessing whether the patterns they unearth are truly useful or meaningless coincidences. While data mining sometimes discovers useful relationships, the data deluge has caused the number of possible patterns that can be discovered relative to the number that are genuinely useful to grow exponentially—which makes it increasingly likely that what data mining unearths is likely to be fool’s gold.;machine learning
Embodied Conversational Agent-Based Kiosk for Automated Interviewing;https://doi.org/10.2753/MIS0742-1222280102;We have created an automated kiosk that uses embodied intelligent agents to interview individuals and detect changes in arousal, behavior, and cognitive effort by using psychophysiological information systems. In this paper, we describe the system and propose a unique class of intelligent agents, which are described as Special Purpose Embodied Conversational Intelligence with Environmental Sensors (SPECIES). SPECIES agents use heterogeneous sensors to detect human physiology and behavior during interactions, and they affect their environment by influencing human behavior using various embodied states (i.e., gender and demeanor), messages, and recommendations. Based on the SPECIES paradigm, we present three studies that evaluate different portions of the model, and these studies are used as foundational research for the development of the automated kiosk. The first study evaluates human-computer interaction and how SPECIES agents can change perceptions of information systems by varying appearance and demeanor. Instantiations that had the agents embodied as males were perceived as more powerful, while female embodied agents were perceived as more likable. Similarly, smiling agents were perceived as more likable than neutral demeanor agents. The second study demonstrated that a single sensor measuring vocal pitch provides SPECIES with environmental awareness of human stress and deception. The final study ties the first two studies together and demonstrates an avatar-based kiosk that asks questions and measures the responses using vocalic measurements.;machine vision
INTELLIGENT ROAD MAINTENANCE: A MACHINE LEARNING APPROACH FOR SURFACE DEFECT DETECTION;https://aisel.aisnet.org/ecis2018_rp/194/;"The emergence of increased sources for Big Data through consumer recording devices gives rise to a new basis for the management and governance of public infrastructures and policy de-sign. Road maintenance and detection of road surface defects, such as cracks, have traditionally been a time consuming and manual process. Lately, increased automation using easily acquirable front-view digital natural scene images is seen to be an alternative for taking timely maintenance decisions; reducing accidents and operating cost and increasing public safety. In this paper, we propose a machine learning based approach to handle the challenge of crack and related defect detection on road surfaces using front-view images captured from driver’s viewpoint under diverse conditions. We use a superpixel based method to first process the road images into smaller coherent image regions. These superpixels are then classified into crack and non-crack regions. Various texture-based features are combined for the classification mod-el. Classifiers such as Gradient Boosting, Artificial Neural Network, Random Forest and Linear Support Vector Machines are evaluated for the task. Evaluations on real datasets show that the approach successfully handles different road surface conditions and crack-types, while locating the defective regions in the scene images.";machine vision
MACHINE LEARNING APPROACHES ALONG THE RADIOLOGY VALUE CHAIN – RETHINKING VALUE PROPOSITIONS;https://aisel.aisnet.org/ecis2019_rp/158/;Radiology is experiencing an increased interest in machine learning with its ability to use a large amount of available data. However, it remains unclear how and to what extent machine learning will affect radiology businesses. Conducting a systematic literature review and expert interviews, we compile the opportunities and challenges of machine learning along the radiology value chain to discuss their implications for the radiology business. Machine learning can improve diagnostic quality by reducing human errors, accurately analysing large amounts of data, quantifying reports, and integrating data. Hence, it strengthens radiology businesses seeking product or service leadership. Machine learning fosters efficiency by automating accompanying activities such as generating study protocols or reports, avoiding duplicate work due to low image quality, and supporting radiologists. These efficiency improvements advance the operational excellence strategy. By providing personnel and proactive medical solutions beyond the radiology silo, machine learning supports a customer intimacy strategy. However, the opportunities face challenges that are technical (i.e., lack of data, weak labelling, and generalisation), legal (i.e., regulatory approval and privacy laws), and persuasive (i.e., radiologists’ resistance and patients’ distrust). Our findings shed light on the strategic positioning of radiology businesses, contributing to academic discourse and practical decision-making.;machine vision
Diagnostic Doubt and Artificial Intelligence: An Inductive Field Study of Radiology Work;https://aisel.aisnet.org/icis2019/future_of_work/future_work/11/;Technological developments in emerging AI technologies are assumed to further routinize and improve the efficiency of decision making tasks, even in professional contexts such as medical diagnosis, human resource management, and criminal justice. We have little research on how AI technologies are actually used and adopted in practice. Prior research on technology in organizations documents a gap between the expectations for new technology and its actual use in practice. We conducted a comparative field study of three sections in a Department of Radiology in a major US hospital, whereby new and existing AI tools were being used and experimented with. In contrast to expectations about AI tools, our study reveals how such tools can lead routine professional decision making tasks to become nonroutine, as they increased ambiguity and decision makers had to work to reduce it. This is particularly challenging since the costs of dealing with ambiguity – increased time to diagnose – were often weighed against the benefits of such ambiguity (potentially more accurate diagnoses). This study contributes to literatures related to technology, work, and organizations, as well as the role of ambiguity in professionals’ knowledge work.;machine vision
Beauty’s in the AI of the Beholder: How AI Anchors Subjective and Objective Predictions;https://aisel.aisnet.org/icis2019/future_of_work/future_work/15/;"Researchers increasingly acknowledge that algorithms can exhibit bias, but artificial intelligence (AI) is increasingly integrated into the organizational decision-making process. How does biased AI shape human choices? We consider a sequential AI-human decision that mirrors organizational decisions; an automated system provides a score and then a human decides a score using their discretion. We conduct an AMT survey and ask participants to assign one of two types of scores: a subjective, context-dependent measure (Beauty) and objective, observer-independent measure (Age). Participants are either shown the AI score, shown the AI score and its error, or not shown the AI score. We find that participants without knowledge of the AI score do not exhibit bias; however, knowing the AI scores for the subjective measure induces bias in the participants’ scores due to the anchoring effect. Although participants’ scores do not display bias, participants who receive information about the AI error rates devalue the AI score and reduce their error. This study makes several contributions to the information systems literature. First, this paper provides a novel way to discuss artificial intelligence bias by distinguishing between subjective and objective measures. Second, this paper highlights the potential spillover effects from algorithmic bias into human decisions. If biased artificial intelligence anchors human decisions, then it can induce bias into previously unbiased scores. Third, we examine a method to encourage participants to reduce their reliance on the artificial intelligence, reporting the error rate, and find evidence that it is effective for the objective measure.";machine vision
The limits to language in doing systems design;https://doi.org/10.1057/s41303-017-0043-4;We employ a conversational genre of performative research in order to explore foundational issues of language and design in information system practice. Initially, Sir Geoffrey Vickers (†2004), C. West Churchman (†1999), Hans-Georg Gadamer (†1982) and Jurgen Habermas are portrayed as engaging in a roundtable discussion on the topic: “Are there Limits to Language in Doing System Design?” We employ an updated, AI-enhanced version of Memex, an intelligent agent originally described by Vannevar Bush at the end of WWII, to serve as a plausible digital platform for enabling a discussion among intelligent agents, both living and dead. The Memex system conducts a spirited conversation among the four scholars and later brings Pierre Bourdieu (†2002) and Bruno Latour into the discussion in order to enrich the unfolding conversation. After the roundtable, Jurgen Habermas and Sir Geoffrey Vickers synthesize the learning from their perspectives.;natural language processing
Needmining: Identifying micro blog data containing customer needs;https://doi.org/10.48550/arXiv.2003.05917;The design of new products and services starts with the identification of needs of potential customers or users. Many existing methods like observations, surveys, and experiments draw upon specific efforts to elicit unsatisfied needs from individuals. At the same time, a huge amount of user-generated content in micro blogs is freely accessible at no cost. While this information is already analyzed to monitor sentiments towards existing offerings, it has not yet been tapped for the elicitation of needs. In this paper, we lay an important foundation for this endeavor: we propose a Machine Learning approach to identify those posts that do express needs. Our evaluation of tweets in the e-mobility domain demonstrates that the small share of relevant tweets can be identified with remarkable precision or recall results. Applied to huge data sets, the developed method should enable scalable need elicitation support for innovation managers - across thousands of users, and thus augment the service design tool set available to him.;natural language processing
DESIGNING AN AI-BASED ADVISORY PLATFORM FOR DESIGN TECHNIQUES;https://aisel.aisnet.org/ecis2019_rp/152/;The usage of design techniques in design processes is an important driver for the success of digital services. However, before using design techniques, suitable techniques need to be selected. With the continuous growth of the number of design techniques, the selection of appropriate ones becomes more difficult, especially for design novices with limited knowledge and expertise. In order to support the selection process, we propose design principles for the development of an advisory platform that interacts with design novices to suggest design techniques for different design situations using artificial intelligence (AI) techniques. Specifically, we leverage conversational agents, recommender techniques, and taxonomic background knowledge to conceptualize and implement an AI-based advisory platform. Following a design science research methodology, we contribute design knowledge for the class of advanced advisory platforms. Furthermore, from a practical point of view, we help design novices with our implemented advisory platform in the contextualized selection process of design techniques.;natural language processing
A Nice and Friendly Chat with a Bot: User Perceptions of AI-Based Service Agents;https://aisel.aisnet.org/icis2017/ServiceScience/Presentations/11/;As more organizations establish artificial intelligence-based service agents to offer an automated customer dialogue it is crucial to understand how users perceive this new form of technology-mediated communication. AI-based service agents interact in a similar way as humans in human-to-human chat conversations, but instead of a live person on the other end, a chat bot steers the communication based on artificial intelligence and Natural Language Interaction. By combining qualitative and quantitative methods, we examine this context and explore the role of perceived authenticity and its impact on users’ attitudinal and behavioral outcomes. Our results from the qualitative studies show that users infer the authenticity of the AI-based service agents based on two different categories of cues: (1) agent-related cues and (2) communication-related cues. We employ additional experimental studies to empirically test antecedents and consequences of authenticity perceptions in AI-based service encounters.;natural language processing
Effects of Voice-Based AI in Customer Service: Evidence from a Natural Experiment;https://aisel.aisnet.org/icis2020/hci_artintel/hci_artintel/21/;"Voice-based AI systems are gradually deployed to replace traditional interactive voice response systems in call centers. However, there is little evidence on how the implementation of AI systems impacts customer behavior as well as the effect of AI systems on call center performance. Using data from a natural field experiment, we examine how the introduction of voice-based AI in a large telecommunication services call center affects call length, customers’ demands for human service, and customer complaints. We find that the implementation of AI significantly increases call length and decreases customer complaints. Although presumably the AI-enhanced service system reduces users’ efforts to transfer to human agents, we do not find a significant increase in customers’ demands for human service. In addition, regarding simple service tasks, the AI-enhanced service system reduces customer complaints for both experienced and inexperienced customers. For relatively complex tasks, customers learn from prior experience of interacting with the AI system; as this learning effect leads to the decrease of complaints. What’s more, our results suggest significant heterogeneity in the effects of the AI-enhanced service system by indicating that the AI-enhanced system has a significantly larger effect on reducing customer complaints for older and female customers as well as for the customers who are experienced in using the IVR system.";natural language processing
"""She is not just a computer"": Gender Role of AI Chatbots in Debt Collection";https://aisel.aisnet.org/icis2020/hci_artintel/hci_artintel/20/;Chatbots have been empowered by Artificial Intelligence (AI) and rapidly applied to many industries. There is a call for more understanding of the effect of chatbots’ social cues on business outcomes. This paper investigates how does the choice of chatbots’ voice gender impacts customers’ intention to repay overdue debt. Prior studies on gender differences have conflicting implications. Applying real business data, we find that for male customers, they are more willing to repay when served by female chatbots. However, female customers have no preference for the gender of chatbots. We finally explain the effect of chatbot gender in ten gender-stereotypical attributes (e.g., forceful and assertive of masculinity, gentle and warm of femininity). The results demonstrate that masculine attributes have negative effects on both male and female customers while feminine attributes only have (positive) effects on male customers. Based on the results, we further discuss the explanation and managerial implications.;natural language processing
The Mind or the Heart? it Depends on the (definition of) Situation;https://doi.org/10.1057/palgrave.jit.2000062;This paper establishes the importance of situatedness of experience in Information Systems (IS) studies, but also critiques the limited notion of situatedness all too frequently employed. In the original language of phenomenology as used by Heidegger, ‘Befindlichkeif means not just ‘state of mind’ but also refers to disposition, mood, affectedness and emotion. The paper reviews the controversies in the literature generated by opponents to the situatedness literature and provides two case studies to show how current IS uses of the situatedness perspectives differ from the original one. From this discussion, the paper argues that the limited IS research agendas on situated action found in Al, cognitive and social sciences need to capture the inner life of the actor, mind and heart, through the scope of a renewed, authentic, phenomenological tradition.;expert systems
Incorporating Software Agents into Supply Chains: Experimental Investigation with a Procurement Task;https://doi.org/10.2307/25148721;Recently, researchers have begun investigating an emerging, technology-enabled innovation that involves the use of intelligent software agents in enterprise supply chains. Software agents combine and integrate capabilities of several information technology classes in a novel manner that enables supply chain management and decision making in modes not supported previously by IT and not reported previously in the information systems literature. Indeed, federations and swarms of software agents today are moving the boundaries of computer-aided decision making more generally. Such moving boundaries highlight promising new opportunities for competitive advantage in business, in addition to novel theoretical insights. But they also call for shifting research thrusts in information systems. The stream of research associated with this article is taking some first steps to address such issues by examining experimentally the capabilities, limitations, and boundaries of agent technology for computer-based decision support and automation in the procurement domain. Procurement represents an area of particular potential for agent-based process innovation, as well as reflecting some of the greatest technological advances in terms of agents emerging from the laboratory. Procurement is imbued with considerable ambiguity in its task environment, ambiguity that presents a fundamental limitation to IT-based automation of decision making and knowledge work. By investigating the comparative performance of human and software agents across varying levels of ambiguity in the procurement domain, the experimentation described in this article helps to elucidate some new boundaries of computer-based decision making quite broadly. We seek in particular to learn from this domain and to help inform computer-based decision making, agent technological design, and IS research more generally.;expert systems
Impact of Artificial Intelligence on Human Decision Making on ICO Platforms;https://aisel.aisnet.org/icis2019/human_computer_interact/human_computer_interact/14/;As artificial intelligence is increasingly used in assisting and augmenting different tasks, it is now imperative to understand how A.I. influences human decision making. Using data collected from a leading ICO evaluation platform, we empirically investigate whether and how human domain experts’ judgments are influenced by A.I. bots. Our preliminary results suggest that experts are consistently influenced by artificial intelligence regardless of whether the project is successful or not, and whether the A.I. bot’s evaluation was correct or wrong. Our results also indicate that going through a Pre-ICO stage reduces information asymmetry and therefore mitigates the influence of A.I. on experts. This paper contributes to the nascent literature in the areas of human-A.I. interaction, A.I.-facilitated decision making and A.I.-induced biases.;expert systems
A Framework for Artificial Knowledge Creation in Organizations;https://aisel.aisnet.org/icis2017/General/Presentations/15/;Recent advances in Artificial Intelligence (AI) have increased the ability of organizations to analyze data to support decisions. However, there is little focus to date, on the potential role of AI in organizational knowledge creation. This paper develops a framework of organizational artificial knowledge creation based on a synthesis of the literature, and the implementation of a multi-agent AI in an organization. We identify five stages for developing organizational artificial knowledge: 1) Extracting and Collecting, 2) Curating, 3) Ingesting, 4) Training and Testing, 5) Analyzing and Predicting. We also identified two main practices triggered by the development of the AI multi-agent that distinguish them from traditional IS: the ability to initiate a dialogue between the different actors which can lead to the consolidation and aggregation of the organizational knowledge, and the ability to establish recursive and reflexive relation between individual knowledge and the organizational artificial knowledge.;expert systems
ME, YOU OR AI? HOW DO WE FEEL ABOUT DELEGATION;https://aisel.aisnet.org/ecis2019_rp/36/;Driven by the growing availability of data and information paired with the human brain’s limited pro-cessing capabilities, Artificial Intelligence (AI) has become an increasingly relevant actor in strategic decision making. To leverage the potential AI provides for strategic decision making, firms rely on managers that are willing to transfer authority and control to AI-based decision systems. By analyzing the willingness to delegate strategic decisions and emotional responses to outcomes of delegated deci-sions, this research contributes to a better understanding of human behavior in decision delegation to AI in the context of strategic decisions. Based on a policy-capturing study, we find that humans are less likely to delegate a strategic decision to AI, than to another person. Further, our findings reveal that emotional responses to the outcomes of delegated decisions are more intense when responsibility was delegated to another human being, rather than to an AI-enabled system.;expert systems
Exploring factors that determine consumer attitude toward use of intelligent software agents;https://aisel.aisnet.org/ecis2006/141/;The aim of the research described in this paper is to evaluate the impact of Intelligent Software Agents (ISA) used in the on-line purchase of e-tickets, and their acceptance as a source of information service and decision assistance for customers. An empirical study was conducted using the Technology Acceptance Model (TAM) and the EC Consumer Behaviour Model in order to determine the factors affecting consumer attitude towards using ISA. The research instrument employed was a questionnaire survey among 150 postgraduate students from the School of Management at the University of Surrey, the findings demonstrate a positive view of ISA – the stronger predictors of consumer attitude towards ISA being perceived usefulness (PU), information richness (IR), customer interface (CI) and perceived trust (PT). Overall, the future of ISA at the business-to-customer (B2C) interface level is to a great extent affected by the consumers’ perception of ISA usefulness and control over it. The present study’s recommendations indicate a need for travel businesses to develop more intelligent electronic environments that will elicit a more positive response from the consumers if they are to be used more.;expert systems
User Interaction with AI-enabled Systems: A Systematic Review of IS Research;https://aisel.aisnet.org/icis2018/general/Presentations/7/;The improved performance of technological capabilities in the field of artificial intelligence (AI), including computer vision and natural language processing, makes it possible to enhance existing and to develop new types of information systems. We refer to such systems as AI-enabled systems. User interaction with these systems is an important topic for information systems (IS) research because they are supposed to bring about substantial change for individuals, organizations, and society. Despite the recent public and academic interest in AI, AI-enabled systems are not a new phenomenon. However, previous research is separated into research streams on different AI-enabled system types. We conducted a literature review to aggregate the dispersed knowledge regarding individual user interaction with such systems in IS research. Our results show common behavioral patterns in interactions between users and various types of AI-enabled systems and provide a solid foundation for future research on this topic.;expert systems
AI Agents for Sequential Promotions: Combining Deep Reinforcement Learning and Dynamic Field Experimentation;https://aisel.aisnet.org/icis2020/hci_artintel/hci_artintel/23/;As the most cutting-edge frontiers, AI agents bring many exciting opportunities for personalized promotions. The current state-of-art myopic targeting methods only try to optimize the current reward and hence ignore what effect the current promotions might bring to the future revenue. AI agents can stand at the company’s perspective and behave as a forward-looking manager which considers current reward and future revenue simultaneously when designing the targeting policy to maximize the company's long-term revenue. In the meanwhile, the AI agent can also account for managerial risk preferences (risk-seeking). In this study, we illustrate the design and implementation of a deep reinforcement learning (DRL)-based AI agent for sequential personalized promotions based on a large field experiment on randomized promotions in a major mobile app platform. The results suggest that AI agents with “risk-seeking” exploration can identify the optimal personalized promotion policy faster and reach a higher long-term revenue for the platform in comparison with a more conservative exploration. In addition, a forward-looking AI agent accounting for 90% of the future delayed reward works best among the different decision horizons. We further demonstrate that our DRL-based AI agent generates 27.8% more long-term revenue compared with non-personalized mass promotions, and 24.3% more long-term revenue compared with various myopic personalized approaches. Finally, we dig into what explains the differences in outcomes and interventions under the proposed dynamic DRL-based AI agent and other benchmark policies, we find that the comparative advantage is driven by incorporating intertemporal trade-offs.;expert systems
Fostering Human Agency: A Process for the Design of User-Centric XAI Systems;https://aisel.aisnet.org/icis2020/hci_artintel/hci_artintel/12/;The emerging research field of Explainable Artificial Intelligence (XAI) addresses the problem that users do not trust or blindly follow AI systems that act as black boxes. XAI research to date is often criticized for not putting the user at the center of attention. Against this background, we design a process to systematically guide the instantiation, calibration, and quality control of XAI systems such that they foster human agency and enable appropriate trust in AI systems. The process can be applied independent of the XAI method, application domain, and target user group. It incorporates the principles of user-centric design, insights into explanations from the social sciences, and established XAI evaluation scenarios. Following the Design Science methodology, we demonstrate the practical applicability of our artifact and evaluate its efficacy in a realistic setting. Our work contributes to the design of user-centric XAI systems and the quest for human agency in AI.;expert systems
Research Perspectives: The Rise of Human Machines: How Cognitive Computing Systems Challenge Assumptions of User-System Interaction;https://aisel.aisnet.org/jais/vol21/iss2/2/;Cognitive computing systems (CCS) are a new class of computing systems that implement more human-like cognitive abilities. CCS are not a typical technological advancement but an unprecedented advance toward human-like systems fueled by artificial intelligence. Such systems can adapt to situations, perceive their environments, and interact with humans and other technologies. Due to these properties, CCS are already disrupting established industries, such as retail, insurance, and healthcare. As we make the case in this paper, the increasingly human-like capabilities of CCS challenge five fundamental assumptions that we as IS researchers have held about how users interact with IT artifacts. These assumptions pertain to (1) the direction of the user-artifact relationship, (2) the artifact’s awareness of its environment, (3) functional transparency, (4) reliability, and (5) the user’s awareness of artifact use. We argue that the disruption of these five assumptions limits the applicability of our extant body of knowledge to CCS. Consequently, CCS present a unique opportunity for novel theory development and associated contributions. We argue that IS is well positioned to take this opportunity and present research questions that, if answered, will lead to interesting, influential, and original theories.;expert systems
The strategic impacts of Intelligent Automation for knowledge and service work: An interdisciplinary review;https://doi.org/10.1016/j.jsis.2020.101600;A significant recent technological development concerns the automation of knowledge and service work as a result of advances in Artificial Intelligence (AI) and its sub-fields. We use the term Intelligent Automation to describe this phenomenon. This development presents organisations with a new strategic opportunity to increase business value. However, academic research contributions that examine these developments are spread across a wide range of scholarly disciplines resulting in a lack of consensus regarding key findings and implications. We conduct the first interdisciplinary literature review that systematically characterises the intellectual state and development of Intelligent Automation technologies in the knowledge and service sectors. Based on this review, we provide three significant contributions. First, we conceptualise Intelligent Automation and its associated technologies. Second, we provide a business value-based model of Intelligent Automation for knowledge and service work and identify twelve research gaps that hinder a complete understanding of the business value realisation process. Third, we provide a research agenda to address these gaps.;expert systems
Service robots in hospitals: new perspectives on niche evolution and technology affordances;https://doi.org/10.1057/s41303-017-0046-1;Changing demands in society and the limited capabilities of health systems have paved the way for robots to move out of industrial contexts and enter more human-centered environments such as health care. We explore the shared beliefs and concerns of health workers on the introduction of autonomously operating service robots in hospitals or professional care facilities. By means of Q-methodology, a mixed research approach specifically designed for studying subjective thought patterns, we identify five potential end-user niches, each of which perceives different affordances and outcomes from using service robots in their working environment. Our findings allow for better understanding resistance and susceptibility of different users in a hospital and encourage managerial awareness of varying demands, needs, and surrounding conditions that a service robot must contend with. We also discuss general insights into presenting the Q-methodology results and how an affordance-based view could inform the adoption, appropriation, and adaptation of emerging technologies.;robotics
Robotics at workplace: An integrated Twitter analytics – SEM based approach for behavioral intention to accept;https://doi.org/10.1016/j.ijinfomgt.2020.102210;Robotics application has provided a fruitful combination for hybrid industry 4.0 teams wherein robotic systems are progressing their way into spaces shared with human workers. The advent of artificial intelligence has also instilled the feeling of distress and ambiguity among employees regarding their future. Such confusion has triggered a plethora of deliberations surrounding the possible receptivity and hostility towards these modern technologies in the digital era where people are articulating their opinions and experiences through social media communication. This study explores the antecedents of employees’ intention to accept robotics at workplace using two-step analyses: Twitter Analysis and Survey-based analysis. Around 121,750 tweets from 43,000 Twitter handles were evaluated in the form of descriptive analysis, geospatial analysis, network analysis and sentiment analysis. Thereafter, the conceptual model has been formulated and validated based on the extracted themes (anthropomorphism, technophobia and behavioral intention to accept robotics at workplace) by including 864 responses from an online survey conducted in India. The findings corroborated that anthropomorphism and technophobia significantly influence behavioral intention, and technophobia acts as a significant competitive mediator.;robotics
EXPLORING CUSTOMERS’ ACCEPTANCE OF AND RESISTANCE TO SERVICE ROBOTS IN STATIONARY RETAIL – A MIXED METHOD APPROACH;https://aisel.aisnet.org/ecis2020_rp/9/;Stationary retailers continue to try to respond to customers’ needs with regard to service offering and quality. Consequently, they are attempting to develop innovative value propositions and co-create value with customers through new technologies. Among other technologies, service robots (SR) are said to have the potential to revitalise interactive value creation in stationary retail. Nonetheless, the integration of such technologies poses new challenges. Use cases are subject to research, but few studies have explored customers’ perceptions of SRs from a service systems’ perspective, though this is crucial to integrating SRs into stationary retail service systems. In this study, a mixed method approach is adopted to explore customers’ acceptance of and resistance to SRs. First, a qualitative exploratory interview study is conducted among 24 customers. Second, a qualitative survey and a quantitative questionnaire are carried out. The findings identify decisive drivers and barriers, i.a. ‘social presence’ and ‘role congruency’ and reveal i.a. that customers envision harmonious human-robot teams with transparent responsibilities that improve service interactions: while SRs assist frontline employees (FLE) and respond to simple customer inquiries, FLEs can dedicate more time engaging with customers and providing professional customer advice. Moreover, customers suggest that SRs be introduced gradually and with FLEs’ qualified assistance.;robotics
